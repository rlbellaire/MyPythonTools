{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- https://www.npr.org/sections/thesalt/2017/11/30/565769194/i-was-poisoned-can-crowdsourcing-food-illnesses-help-stop-outbreaks <br>\n",
    "- https://clark.com/shopping-retail/food-restaurants/i-was-poisoned-website-restaurant-reports/ <br>\n",
    "- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5517822/ <br>\n",
    "\n",
    "### Software Extensions\n",
    "- Current implementation uses Beautiful Soup which should work until IWP.com changes their portal format\n",
    "- Scrapy framework was identified late. In future may choose to build web spider https://scrapy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import glob\n",
    "import copy\n",
    "import string\n",
    "import base64\n",
    "import string\n",
    "import hashlib\n",
    "import subprocess\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "\n",
    "import urllib2\n",
    "import requests\n",
    "import usaddress\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from HTMLParser import HTMLParser\n",
    "from collections import defaultdict\n",
    "from datetime import date, timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "def stripspaces(text):\n",
    "    return re.sub(' +', ' ', text, flags=re.UNICODE).rstrip()\n",
    "\n",
    "def merged_states(row):\n",
    "    if pd.isnull(row[0]):\n",
    "        return np.nan\n",
    "    if len(row[0]) == 2:\n",
    "        return row[0]\n",
    "    else:\n",
    "        return row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish Hive table columns and schema formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = [u'uid', u'source', 'IWP_brand', 'Brand_prefix', u'date_time', u\"Report Type\",\n",
    "           u'Symptoms', u'Diagnosis', u'Doctor Visit', u'Details', u'AddressNumber', u'BuildingName',\n",
    "           u'CountryName', u'Location', u'PlaceName', u'StateName', u'StreetName', u'StreetNamePostDirectional',\n",
    "           u'StreetNamePostType', u'StreetNamePreDirectional', u'ZipCode']\n",
    "\n",
    "my_index = 'uid'\n",
    "\n",
    "my_formats = {\"uid\": unicode, \"source\": unicode, \"IWP_brand\":unicode, 'Brand_prefix': unicode,\n",
    "    \"date_time\": pd.to_datetime, \"Report Type\": unicode, \"Symptoms\": unicode, \"Diagnosis\":unicode,\n",
    "    \"Doctor Visit\":unicode, \"Details\":unicode, \"AddressNumber\":str, \"BuildingName\":unicode,\n",
    "    \"CountryName\":unicode, \"Location\":unicode, \"PlaceName\":unicode, \"StateName\":unicode,\n",
    "    \"StreetName\":unicode, \"StreetNamePostDirectional\":unicode, \"StreetNamePostType\":unicode,\n",
    "    \"StreetNamePreDirectional\":unicode, \"ZipCode\":unicode}\n",
    "\n",
    "data_path = \"/Users/loubellaire/Documents/_SMG/Epic_Planning/DataScience/Text Processing/Alerts/food_illness/IWP_tableformat/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load reference data\n",
    "- Geographic state name standards and (FIPS) values\n",
    "- For future -- spell correct the brand names using SMG dictionaries via GitHub (norm_source pull request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fips = pd.read_excel(data_path + \"US_States_FIPS_codes.xlsx\")\n",
    "df_fips = df_fips.rename(columns = lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "norm_source = \"https://raw.github.com/ServiceManagementGroup/datascience-dictionaries/blob/master/entities/Brands/normalization.dat\"\n",
    "requests.get(norm_source, auth=('rlbellaire', '1234qwer')) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map IWasPoisoned.com Brands\n",
    "- mapping Iwaspoisoned.com (IWP) brands to SMG_KC brand prefix or (if not client) SMini parent brand.\n",
    "- Do some normalization of IWP brand names to get more accurate counts/brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format IWP: SMG (VV code or SM Parent brand)\n",
    "\n",
    "brand_prefix_dict ={\"IHOP\": \"DINE_IHOP\", u\"7-Eleven\": u\"7-Eleven\",\n",
    "    u'99 Cents Only Stores': u\"99 Cents Only Store\", \"AMC\": \"AMC\", u'ALDI': u\"Aldi\", \n",
    "    u'Applebee\\u2019s': \"DINE_APP\", u\"Arby's\" : u\"Arby's Restaurant Group, Inc.\",\n",
    "    u\"Auntie Anne's Pretzels\": \"Auntie Anne's\", u'Bahama Breeze' : \"DAR\", \n",
    "    u\"Chipotle Mexican Grill\": u\"Chipotle\", u\"McDonald's\": u\"MCD\", u'Taco Bell': u\"TAC\",\n",
    "    u\"Wendy's\": \"WEND\", u'Burger King' : u\"BK\", u'Chick-fil-A': u\"CFA\",\n",
    "    u'SUBWAY\\xaeRestaurants': \"Doctor's Associates, Inc.\", u'Starbucks': \"STAR\",\n",
    "    u'Panera Bread': u\"Panera\", u'Panda Express': \"Panda Express\",\n",
    "    u'Buffalo Wild Wings': \"BUFA\", u'Pizza Hut': \"PHI\", u'KFC': u\"YRD_KF\", \n",
    "    u'Sonic Drive-In': u\"SRI\", u'Popeyes Louisiana Kitchen': u\"POP\",\n",
    "    u'In-N-Out Burger': u\"In-N-Out Burger\",  u'Little Caesars Pizza': \"Little Caesars\",\n",
    "    u\"Domino's Pizza\": u'DOM', u\"Chili's Grill & Bar\": \"Brinker International\",\n",
    "    u'Olive Garden Italian Restaurant': u'DAR', u'Jack In The Box': \"JACK\",\n",
    "    u\"Jimmy John's\": u\"Jimmy John's\", u'Red Lobster': \"REDL\",\n",
    "    u\"Moe's Southwest Grill\": u\"Moe's Southwest Grill\", u'Texas Roadhouse': u\"Texas Roadhouse\",\n",
    "    u'Golden Corral': u\"Golden Corral\", u\"Dunkin' Donuts\": u\"DUN\",\n",
    "    u\"Denny's\": u\"DENN\", u'QDOBA Mexican Eats': \"QDO\", u'Wingstop': \"Roark Capital Group\",\n",
    "    u'El Pollo Loco': u\"El Pollo Loco\", u\"Raising Cane's Chicken Fingers\": u\"Raising Cane's\",\n",
    "    u'The Cheesecake Factory': u\"CHE\", u\"Zaxby's Chicken Fingers & Buffalo Wings\": u\"Zaxby's\",\n",
    "    u'Whataburger': 'WHT', u'Five Guys': u\"Five Guys\", u'Del Taco': \"Del Taco\",\n",
    "    u'Waffle House': u\"Waffle House, Inc.\", u'MOD Pizza': u\"MOD Pizza\", u'Potbelly Sandwich Shop': u\"POT\",\n",
    "    u\"Bojangles' Famous Chicken 'n Biscuits\": u\"BOJA\", u'Baskin-Robbins': u\"BR\",\n",
    "    u'Bonefish Grill': u\"OUTB_BON\", u'California Pizza Kitchen': u\"CPKI\", u\"Captain D's\": u\"CAP\",\n",
    "    u\"Carl's Jr.\": u\"CKE Restaurants\", u\"Carrabba's Italian Grill\": u\"OUTB_CAR\",\n",
    "    u\"Casey's General Store\": u\"Casey's General Store\", u'Checkers': u\"CHEC\", u\"Church's Chicken\": u\"CHU\",\n",
    "    u'Corner Bakery Cafe': u\"CBC\", u'Dairy Queen': \"DAI\", u\"Eddie V's Prime Seafood\": \"DAR\",\n",
    "    u\"Famous Dave's Bar-B-Que\": u\"FAMD\", u\"Fazoli's\": u\"FAZ\",\n",
    "    u'Firehouse Subs': u\"FIRE\", u\"Friendly's\": u\"FRIE\", u\"Hardee's\": u\"CKE Restaurants\",  u'Jamba Juice': u\"JAMB\",\n",
    "    u\"Jason's Deli\": u\"Jason's Deli\", u\"Jersey Mike's Subs\": u\"Jersey Mike's Subs\",\n",
    "    u\"Jimmy John's\": u\"Jimmy John's\", u'Krispy Kreme Doughnuts': u\"KRI\", u'Kroger': u\"KRO\",\n",
    "    u\"Logan's Roadhouse\": u\"LORH\", u\"Long John Silver's\": u\"LONG\", u'LongHorn Steakhouse': u\"DAR\",\n",
    "    u'Noodles and Company': u\"NOO\", u'On The Border Mexican Grill & Cantina': u\"On The Border\",\n",
    "    u'Outback Steakhouse': u\"OUTB\", u\"P.F. Chang's\": u\"PFC\", u'Panda Express': u\"Panda Express\",\n",
    "    u\"Papa Murphy's Take 'N' Bake Pizza\": u\"Papa Murphy's\", u'Pei Wei': u\"PFC\",u'Pollo Tropical': u\"FIES_TROP\",\n",
    "    u'QuickChek': u\"QuickChek\", u'QuikTrip': u\"QuikTrip\", u\"Rally's\": u\"CHEC\", \n",
    "    u'Red Robin Gourmet Burgers': u\"REDR\", u'Regal Cinemas': u\"Regal Entertainment Group\",\n",
    "    u\"Romano's Macaroni Grill\": u\"Ignite Restaurant Group\", u'Shake Shack': u\"Shake Shack\",\n",
    "    u'Smashburger': u\"Smashburger\", u'Steak N Shake': u\"Steak 'n Shake\", \n",
    "    u\"Ted's Montana Grill\": u\"Ted's Montana Grill\", u'Zo\\xebs Kitchen': u\"Zo\\xebs Kitchen\",\n",
    "    u\"White Castle\": u\"White Castle\", u\"Wawa\": u\"WAWA\", u\"Culver's\": u\"CULV\", u\"Krystal\": u\"KRY\",\n",
    "    u\"Taco Cabana\": u\"FIES_TACO\", u\"Boston Market\": u\"BOSM\", u\"Ruby Tuesday\": u\"RUBY\", \n",
    "    u\"Baja Fresh\": u\"Fresh Enterprises, LLC\", u\"Yard House\": u\"DAR\", u\"Safeway\": u\"SAFE\", u\"vons\": u\"SAFE\",\n",
    "    u\"Cheddar's Scratch Kitchen\":u\"Cheddar's\", u\"Portillo's Hot Dogs\": u\"PORT\",\n",
    "    u\"BJ's Restaurant & Brewhouse\": u\"BJS Restaurant & Brewhouse\", u\"Trader Joe's\": u\"Trader Joe's\",\n",
    "    u\"Whole Foods Market\": u\"Whole Foods\", u\"Pret A Manger\": u\"Pret a Manger UK\", \n",
    "    u\"Sheetz\": u\"Shee\", \"Joe's Crab Shack\": \"JOES\", u\"Schlotzsky's\": u\"Focus Brands\",\n",
    "    u\"Cracker Barrel Old Country Store\": u\"CRA\", u\"Houlihan's\": u\"HOU\", \"marco's pizza\": u\"MARC\",\n",
    "    u\"which wich superior sandwiches\": u\"WHIC\", u\"Papa John's Pizza\": u\"PAPA\", u'Wienerschnitzel': u\"GALR\",\n",
    "    u\"einstein bros. bagels\": u\"EIN\", u\"tropical smoothie cafe\": u\"TROP\", \"meijer\": u\"MEIJ\", \n",
    "    u\"caribou coffee\": u\"CARI\"}\n",
    "           \n",
    "brand_prefix_dict = {k.lower():v + \"_IWP\" for (k,v) in brand_prefix_dict.items()}\n",
    "\n",
    "brand_variation_dict = {u'Chick-fil-A': [u'Chick-fil-A Avalon', u'Chick-fil-A Bangor',\n",
    "                                         u'Chick-fil-A Eastern and Ione', u'Chick-fil-A Mowry Ave',\n",
    "                                         u'Chick-fil-A North Decatur', u'Chick-fil-A Pearland Parkway',\n",
    "                                         u'Chick-fil-A Tampa Stadium', u'Chick-fil-A Westnedge',\n",
    "                                         u'Chick-fil-A Worcester', u'Chick-fil-A at Glendora Commons',\n",
    "                                         u'Chick-fil-A at Melrose Park', u\"chick fil a\",\n",
    "                                         u\"chick-fil-a at northeast tower center\"],\n",
    "            u'California Pizza Kitchen': [u'California Pizza Kitchen at Hill Center',\n",
    "                                          u'California Pizza Kitchen at Willow Bend',],\n",
    "            u'Jamba Juice': [u'Jamba Juice Amerige Center', u'Jamba Juice Larchmont Ave',],\n",
    "            u'Shake Shack': [u'Shake Shack - Coral Gables', u'Shake Shack - Kierland Commons',\n",
    "                  u'Shake Shack - MGM NY NY', u'Shake Shack - West Loop', u'Shake Shack - Winter Park',],\n",
    "            u\"Bojangles' Famous Chicken 'n Biscuits\": [u\"Bojangles' Coliseum\"],\n",
    "            u'Dairy Queen': [u'Dairy Queen Grill & Chill - Cary', \"DQ Grill & Chill\", \"dairy queen (treat only)\"],\n",
    "            u'Krispy Kreme Doughnuts':[ u'Krispy Kreme Evergreen Park'],\n",
    "            u'SUBWAY\\xaeRestaurants': [u'Subway', u\"subway resturant inside chi\", u\"subway® restaurants\",\n",
    "                                       u\"subway south commercial\"],\n",
    "            u'Steak N Shake': [\"Steak 'n Shake\"], u\"Whole Foods Market\": [u\"whole foods\"],\n",
    "            u\"Chipotle Mexican Grill\": [\"Chipotle\", u\"chipotle\", \"chipotle mexican grill .\"],\n",
    "            u\"Souplantation\": [\"Garden Fresh\"], u'Kroger': [u\"kroger marketplace\"],\n",
    "            u\"Raising Cane's Chicken Fingers\": [\"Raising Cane's\"],\n",
    "            u'Applebee\\u2019s': [u\"applebee’s grill + bar\", u\"applebee's\", u\"applebee’s grill + bar\",\n",
    "                                 u\"applebee\\u2019s grill + bar\"],\n",
    "            u\"AMC\":[\"amc hoffman center 22\", \"amc classic apple blossom 12\"],\n",
    "            u\"Chili's Grill & Bar\" :[u\"chili's grill & bar hagerstown\"],\n",
    "            u'Pizza Hut': [u\"pizza hut express\"], u\"Wawa\":[\"wawa 4455 sr 64 east\"],\n",
    "            u\"Papa John's Pizza\":[u'Papa Johns', u\"papa john's pizza\", u\"papa john\\u2019s pizza\"],\n",
    "            u'Starbucks':[u\"starbucks reserve\"], u'QDOBA Mexican Eats': [u\"qdoba mexican grill\"],\n",
    "            u\"Carl's Jr.\": [u\"carl's jr. / green burrito\"], u'Golden Corral': [u\"golden corral elyria\"],\n",
    "            }\n",
    "\n",
    "brand_normalization = {}\n",
    "for k,values_list in brand_variation_dict.items():\n",
    "    for value in values_list:\n",
    "        brand_normalization[value.lower()] = k.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query SMG Database for Historical IWP.com Reports\n",
    "- Current: read all past records in from .csv \n",
    "- Future: think we just need legacy_stop_date from the historical data. Need to confirm no duplicates could possibly result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/loubellaire/Documents/_SMG/Epic_Planning/DataScience/Text Processing/Alerts/food_illness/IWP_tableformat/iwaspoisoned_scraped20180919_1805.xlsx\n",
      "7023\n"
     ]
    }
   ],
   "source": [
    "input_file = \"iwaspoisoned_scraped*.xlsx\"\n",
    "\n",
    "list_of_files = glob.glob(data_path + input_file) \n",
    "if len(list_of_files)>0:\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print latest_file\n",
    "    df_ht_iwp_legacy_raw = pd.read_excel(latest_file, 'IWP_Raw', converters = my_formats)\n",
    "    df_ht_iwp_legacy_raw = df_ht_iwp_legacy_raw[my_cols]\n",
    "    \n",
    "print len(df_ht_iwp_legacy_raw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IWP.com Webspider Code\n",
    "- Most recent 30-days of reporting is available via IWP.com website \n",
    "- reports which include consumer complaints as well as health department reports (identified as Report_Type = \"Temporary Closure\")\n",
    "- This function scrapes IWP.com website to fill in gaps between today and last database update\n",
    "- A UID is created format is Date_IWP_hash(location)\n",
    "- We use usaddress.py (open source library) to parse address information. Implementation is best available however it is less than perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-19 12:57:00\n",
      "page: 73, error count: 1\n",
      "Date exceeded 2018-09-19 09:42:00\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple Test Code:\n",
    "       r = requests.get('https://iwaspoisoned.com/?page=1')\n",
    "       soup_0 = BeautifulSoup(r.text, 'html.parser') # \"lxml\")\n",
    "\n",
    "webspider termination test\n",
    "\n",
    "now = datetime.now()\n",
    "df_ht_iwp_legacy['delta'] = now - df_ht_iwp_legacy[\"date_time\"].astype(datetime)\n",
    "legacy_stop_date = df_ht_iwp_legacy[df_ht_iwp_legacy['delta'] == min(df_ht_iwp_legacy['delta'])][\"date_time\"][0]\n",
    "\n",
    "ambiguous_rec_date = datetime.strptime(attributes_dict[\"date_time\"], '%b %d %H:%M%p')\n",
    "candidates_rec_dto = [ambiguous_rec_date.replace(year=datetime.now().year),\\\n",
    "                      ambiguous_rec_date.replace(year=(datetime.now()-relativedelta(years=1)).year)]\n",
    "current_rec_dto = nearest(candidates_rec_dto, now)\n",
    "\n",
    "print (current_rec_dto - stop_date)\n",
    "\n",
    "if current_rec_dto >= stop_date:\n",
    "    continue\n",
    "else:\n",
    "    break\n",
    "'''\n",
    "\n",
    "stamper = lambda s: unicode(s.strftime('%Y%m%d%H%M%S') + \"IWP\")\n",
    "hasher = lambda s: base64.urlsafe_b64encode(hashlib.sha256(s.encode('utf-8')).hexdigest()[0:26])\n",
    "date_coverage = df_ht_iwp_legacy_raw[\"date_time\"].map(lambda x: x.strftime('%Y-%m-%d')).unique()\n",
    "\n",
    "now = datetime.now()\n",
    "df_ht_iwp_legacy_raw['delta'] = now - df_ht_iwp_legacy_raw[\"date_time\"].astype(datetime)\n",
    "legacy_stop_date = df_ht_iwp_legacy_raw[df_ht_iwp_legacy_raw['delta'] ==\\\n",
    "                                        min(df_ht_iwp_legacy_raw['delta'])][\"date_time\"].tolist()[0]\n",
    "print legacy_stop_date\n",
    "\n",
    "data_list = [] \n",
    "count = 0\n",
    "for i in xrange(1,366):\n",
    "    time.sleep(7)\n",
    "    sit_reports = []\n",
    "    try:\n",
    "        r = requests.get('https://iwaspoisoned.com/?page=' + str(i))\n",
    "        soup = BeautifulSoup(r.text, 'html.parser') # \"lxml\")\n",
    "        #sit_reports = soup.find_all('div', class_=\"post-box\")\n",
    "        sit_reports = soup.find_all('div', class_=\"div-report-box-small\")\n",
    "    \n",
    "    except:\n",
    "        time.sleep(80)\n",
    "        count +=1\n",
    "        print \"page: {0}, error count: {1}\".format(i, count)\n",
    "        if count > 32:\n",
    "            break\n",
    "\n",
    "    for report in sit_reports:\n",
    "        attributes_dict = {}\n",
    "        if u'USA' in report.find('div', class_ = \"col-md-6 report-first-box pull-left\").find('a').get_text():\n",
    "            ambiguous_rec_date = report.find('p', class_ = \"report-date\").get_text()\n",
    "            ambiguous_rec_date = datetime.strptime(ambiguous_rec_date, '%b %d %H:%M%p')\n",
    "            candidate_rec_dtos = [ambiguous_rec_date.replace(year=datetime.now().year),\\\n",
    "                                  ambiguous_rec_date.replace(year=(datetime.now()-relativedelta(years=1)).year)]\n",
    "            attributes_dict['date_time'] = nearest(candidate_rec_dtos, now)\n",
    "            if attributes_dict['date_time'] < legacy_stop_date:\n",
    "                print \"Date exceeded\", attributes_dict['date_time']\n",
    "                break\n",
    "            s = report.find('h3', class_= \"report-box-h\").get_text()\n",
    "            attributes_dict['Location'] = s\n",
    "            attributes_dict['IWP_brand'] = s.split(\",\")[0]\n",
    "            attributes_dict['source'] = \"iwaspoisoned.com\"\n",
    "            \n",
    "            if report.find('p', class_= \"report-tag\") is not None:\n",
    "                t = report.find('p', class_= \"report-tag\").get_text()\n",
    "                attributes_dict['Symptoms'] = t.split(\":\")[1].strip()\n",
    "                attributes_dict['Report Type'] = \"Food Poisoning\"\n",
    "                attributes_dict[\"StreetNamePostDirectional\"] = None\n",
    "                attributes_dict[\"Details\"] = report.find(\"div\", class_=\"col-md-6 report-second-box pull-left\").find(\"p\").get_text()\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            my_odict =  OrderedDict({})\n",
    "            try:\n",
    "                my_odict = usaddress.tag(','.join(s.split(',')[1:-1]))[0]\n",
    "            except usaddress.RepeatedLabelError as e :\n",
    "                LandmarkName_list = [item[0] for item in e.parsed_string if item[1] == 'LandmarkName']\n",
    "                if len(LandmarkName_list) > 1:\n",
    "                    my_odict[\"LandmarkName\"] = ' '.join(LandmarkName_list)\n",
    "                StateName_list = [item[0] for item in e.parsed_string if item[1] == 'StateName']\n",
    "                if len(StateName_list) > 1:\n",
    "                    my_odict[\"StateName\"] = min(StateName_list, key=len)\n",
    "                PlaceName_list = [item[0] for item in e.parsed_string if item[1] == 'PlaceName_list']\n",
    "                if len(PlaceName_list) > 1:\n",
    "                    my_odict['PlaceName'] = PlaceName_list[0]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            v = None\n",
    "            for k,v in my_odict.items():\n",
    "                attributes_dict[k] = v\n",
    "            data_list.append(copy.deepcopy(attributes_dict))\n",
    "            attributes_dict.clear()\n",
    "            my_odict.clear()\n",
    "    else:                                 # see: http://psung.blogspot.com.au/2007/12/for-else-in-python.html\n",
    "        continue\n",
    "    break\n",
    "\n",
    "data = pd.DataFrame(data_list)\n",
    "drop_list = list(set(data.columns.values).intersection(set(['CornerOf', 'LandmarkName',\\\n",
    "                                                            'Recipient', u'Closure Date', 'NotAddress'])))\n",
    "data = data.drop(drop_list, axis=1)\n",
    "cols_withnulls = ['Diagnosis', 'Doctor Visit', 'BuildingName', 'CountryName']\n",
    "data[cols_withnulls] = pd.DataFrame([[None, None, None, None]], index=data.index)  \n",
    "\n",
    "data['IWP_brand'] = data['IWP_brand'].apply(stripspaces).str.lower()\n",
    "data['IWP_brand'] = data['IWP_brand'].replace(brand_normalization)\n",
    "\n",
    "data['Brand_prefix'] = data['IWP_brand'].str.lower().map(brand_prefix_dict)\n",
    "data['Brand_prefix'] = data['Brand_prefix'].where((pd.notnull(data['Brand_prefix'])), None)\n",
    "\n",
    "#df['source'] = \"iwaspoisoned.com\"\n",
    "#df['CountryName'] = 'USA'\n",
    "#data['date_time'] = pd.to_datetime(data['date_time'], format='%b %d %H:%M%p %Y')\n",
    "\n",
    "data['stamped'] = data['date_time'].apply(stamper)\n",
    "data['hash'] = data['Location'].apply(hasher)\n",
    "data['uid'] = data[['stamped','hash']].apply(lambda x: ''.join(x.dropna().astype(unicode)),axis=1)\n",
    "\n",
    "missing_cols = list(set(my_cols) - set(data.columns))\n",
    "if len(missing_cols)>0:\n",
    "    print \"bingo\"\n",
    "    data = data.reindex(columns=my_cols, fill_value=np.nan)\n",
    "    \n",
    "df = data[~data[\"Report Type\"].str.contains(\"Temporary Closure\")]\n",
    "\n",
    "df = df[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Geo's and enrich IWP.com\n",
    "- Future to look at google api to correct usaddress deficiencies.\n",
    "- Add state FIPS code to enable map displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7467\n"
     ]
    }
   ],
   "source": [
    "if df_ht_iwp_legacy_raw.index.name == 'uid':\n",
    "    df_ht_iwp_legacy_raw.reset_index(inplace = True)\n",
    "if df.index.name =='uid':\n",
    "    df.reset_index(inplace = True)\n",
    "\n",
    "df2 = df[~df['uid'].isin(df_ht_iwp_legacy_raw['uid'])].dropna(how = 'all')\n",
    "df_ht_iwp_legacy_update = pd.concat([df_ht_iwp_legacy_raw, df2])\n",
    "df_ht_iwp_legacy_update = df_ht_iwp_legacy_update[my_cols]\n",
    "\n",
    "df_correct = pd.merge(df_ht_iwp_legacy_update, df_fips[['State','USPS']],\\\n",
    "                                   left_on='StateName', right_on= 'State', how='left')\n",
    "df_correct['USPS2'] = df_correct[['StateName', \"USPS\"]].apply(merged_states, axis = 1)\n",
    "df_correct.drop(['StateName', 'State', \"USPS\"], axis=1, inplace = True)\n",
    "df_correct.rename(columns ={'USPS2':'StateName'}, inplace = True)\n",
    "df_final = pd.merge(df_correct,df_fips[['USPS','FIPS']], left_on='StateName', right_on='USPS', how='left')\n",
    "df_final.drop([\"USPS\"], axis=1, inplace = True)\n",
    "\n",
    "print len(df_final.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update SMG Historical Database of scraped IWP.com Reports\n",
    "- Current: write all records out to .csv \n",
    "- Future: udpate Hive table with new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['IWP_brand'] = df_final['IWP_brand'].apply(stripspaces).str.lower()\n",
    "df_final['IWP_brand'] = df_final['IWP_brand'].replace(brand_normalization)\n",
    "\n",
    "df_final['Brand_prefix'] = df_final['IWP_brand'].str.lower().map(brand_prefix_dict)\n",
    "df_final['Brand_prefix'] = df_final['Brand_prefix'].where((pd.notnull(df_final['Brand_prefix'])), None)\n",
    "df_final.sort_values(by = \"date_time\", ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-03 10:37:19.548990\n",
      "Number of new Entries: 444 \n",
      "2018-04-25 09:23:00\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/loubellaire/Documents/_SMG/Epic_Planning/DataScience/Text Processing/Alerts/food_illness/IWP_tableformat/\"\n",
    "output_file = \"iwaspoisoned_scraped{date:%Y%m%d_%H%M}.xlsx\".format(date=datetime.now())\n",
    "\n",
    "writer = pd.ExcelWriter(data_path + output_file, engine='xlsxwriter', options={'encoding':'utf-8'})\n",
    "df_final.to_excel(writer, sheet_name='IWP_Raw')\n",
    "writer.save()\n",
    "\n",
    "print datetime.now()\n",
    "print \"Number of new Entries: {} \".format(len(df.index))\n",
    "print df_final['date_time'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/loubellaire/Documents/_SMG/Epic_Planning/DataScience/Text Processing/Alerts/food_illness/IWP_tableformat/iwaspoisoned_scraped20181003_1037.xlsx\n",
      "2018-04-25 09:23:00\n"
     ]
    }
   ],
   "source": [
    "print data_path + output_file\n",
    "print df_final['date_time'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT INTO DATABASE: Prepare data to be added to the Hive database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes about HDFS\n",
    "#   It is a distributed file system, so you cannot use standard unix commands like ls, cd. Instead\n",
    "# there is a group of commands with a prefix hdfs to help navigate and view the directory structure.\n",
    "# For example, to navigate to a subfolder you would cmd = 'hdfs cd ' + hdfs_path or 'hdfs ls ' + hdfs path.  \n",
    "# To recursively execute a chmod or mkdir rm, you may need to add hdfs -r to the command.\n",
    "\n",
    "# Hive notes: ticks should wrap each object, not the fully-qualified name\n",
    "# for example, c.execute(\"DROP TABLE `default`.`bk_comment_word_freq_with_stops`\")\n",
    "\n",
    "hdfs_path = \"/user/jupyter/LFHC_alerts/foodborne_illness/\"\n",
    "\n",
    "cmd='hdfs dfs -mkdir ' + hdfs_path\n",
    "result=subprocess.check_output(cmd, shell=True)\n",
    "print result\n",
    "\n",
    "cmd='hdfs dfs -chmod 777 ' + hdfs_path\n",
    "result=subprocess.check_output(cmd, shell=True)\n",
    "print result\n",
    "\n",
    "# View directory contents\n",
    "cmd='hdfs dfs -ls ' + hdfs_path\n",
    "result = subprocess.check_output(cmd, shell=True)\n",
    "print;print result\n",
    "\n",
    "with pyhs2.connect(host=hiveHost, port=hivePort, authMechanism=\"PLAIN\", user='elemmon') as cn:\n",
    "    with cn.cursor() as c:\n",
    "        c.execute(\"\"\" CREATE EXTERNAL TABLE `{0}`(\n",
    "                `uid` string,\n",
    "                `source` string,\n",
    "                `IWP_brand` string,\n",
    "                `Brand_prefix` string,\n",
    "                `date_time` TIMESTAMP,\n",
    "                `Report Type` string,\n",
    "                `Symptoms` string,\n",
    "                `Diagnosis` string,\n",
    "                `Doctor Visit` string,\n",
    "                `Details` string,\n",
    "                `AddressNumber` string,\n",
    "                `BuildingName` string,\n",
    "                `CountryName` string,\n",
    "                `Location` string,\n",
    "                `PlaceName` string,\n",
    "                `StreetName` string,\n",
    "                `StreetNamePostDirectional` string,\n",
    "                `StreetNamePostType` string,\n",
    "                `StreetNamePreDirectional` string,\n",
    "                `ZipCode` int,\n",
    "                `FIPS` int,) \n",
    "             ROW FORMAT SERDE\n",
    "             'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "             WITH SERDEPROPERTIES (\n",
    "             'field.delim'=',',\n",
    "             'serialization.format'=',') \n",
    "             STORED AS INPUTFORMAT \n",
    "             'org.apache.hadoop.mapred.TextInputFormat'\n",
    "             OUTPUTFORMAT\n",
    "             'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "             LOCATION\n",
    "             'hdfs://smgdev\"\"\".format(df_final) + hdfs_path + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print brand_prefix_dict[u\"chili's grill & bar\"]\n",
    "print \"olive garden italian restaurant\", \"buffalo wild wings\", \"applebee’s\", \"papa john's pizza\", \"ihop\", \"pizza hut\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to address Historical Data deficiencies\n",
    "```\n",
    "df_ht_iwp_legacy_raw.rename({'brand': 'IWP_brand'}, axis='columns', inplace = True)\n",
    "df_ht_iwp_legacy_raw['IWP_brand'] = df_ht_iwp_legacy_raw['IWP_brand'].apply(stripspaces).str.lower()\n",
    "\n",
    "df_ht_iwp_legacy_raw['IWP_brand'] = df_ht_iwp_legacy_raw['IWP_brand'].replace(brand_normalization)\n",
    "\n",
    "df_ht_iwp_legacy_raw['Brand_prefix'] = df_ht_iwp_legacy_raw['IWP_brand'].map(brand_prefix_dict)\n",
    "df_ht_iwp_legacy_raw['Brand_prefix'] = df_ht_iwp_legacy_raw['Brand_prefix'].\\\n",
    "                                           where((pd.notnull(df_ht_iwp_legacy_raw['Brand_prefix'])), None) \n",
    "df_ht_iwp_legacy_raw['CountryName'] = 'USA'\n",
    "df_ht_iwp_legacy_raw.drop([u'relevant', u'products'], axis=1, inplace=True)\n",
    "\n",
    "df_ht_iwp_legacy_raw =df_ht_iwp_legacy_raw[my_cols]\n",
    "\n",
    "mypath = \"/Users/loubellaire/Documents/_SMG/Epic_Planning/DataScience/Text Processing/Alerts/food_illness/IWP_tableformat/\"\n",
    "output_file = \"iwaspoisoned_scraped{date:%Y%m%d_%H%M}.xlsx\".format(date=datetime.now())\n",
    "\n",
    "writer = pd.ExcelWriter(mypath + output_file, engine='xlsxwriter', options={'encoding':'utf-8'})\n",
    "df_ht_iwp_legacy_raw.to_excel(writer, sheet_name='SourceData')\n",
    "writer.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
