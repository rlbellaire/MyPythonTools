{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker Rank Check List\n",
    "- open https://regex101.com/\n",
    "- open sample notebooks on local machine (iwaspoisoned3.ipynb - requests, TimeSeries_InterpExtrap, Practice_interp, bank_marketing)\n",
    "- open GitHub repo\n",
    "- open local notebook for testing -- copy standard template imports from this notebook\n",
    "- Set up a timer so you can track your progress against the time limits\n",
    "- Be ready to copy standard imports from this template into HackerRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Template for HackerRank\n",
    "- Note hacker rank doesn't seem to allow:\n",
    "       - import of IPython to display dataframes\n",
    "       - %matplotlib inline\n",
    "       - import matplotlib.pyplot as plt\n",
    "       - import matplotlib.dates as mdates\n",
    "       - from pylab import rcParams\n",
    "       - import matplotlib.pyplot as plt\n",
    "       - import matplotlib.dates as mdates\n",
    "       - from pylab import rcParams\n",
    "       - rcParams['figure.figsize'] = 10, 6\n",
    "       - plt.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "plt.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for HackerRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import *\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict, OrderedDict\n",
    "import itertools\n",
    "from dateutil import parser\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "import sympy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import mode\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn import preprocessing, linear_model, metrics\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# if __name__ == \"__main__\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HackerRank I/O\n",
    "## Notes on HackerRank STDIn and STDOut\n",
    "- https://www.hackerrank.com/challenges/solve-me-first/problem\n",
    "\n",
    "## Reading a local CSV file\n",
    "- lineterminator='\\r'\n",
    "- sep = ',', '\\t'\n",
    "- parse_dates = [0] # date column\n",
    "- infer_datetime_format = True,\n",
    "- date_parser = pd.to_datetime\n",
    "\n",
    "### Set time-based index\n",
    "- df_pass.set_index(\"month\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "num1 = int(input())\n",
    "num2 = int(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickly load sample data to local machine (via copy/paste clipboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and loading local CSV's\n",
    "- Date Time codes: https://docs.python.org/2/library/datetime.html\n",
    "        Code\tMeaning\tExample\n",
    "        %A\tWeekday as full name.\t      Wednesday\n",
    "        %a\tWeekday as abbrev. name:\t    Wed\n",
    "        %B\tMonth as localeâ€™s full name.\tJune\n",
    "        %b  Abbreviated month name:         Jun\n",
    "        %d\tDay of the month.\t            06\n",
    "        %m\tMonth as a number.\t             6\n",
    "        %Y\tFour-digit year.\t           2018\n",
    "        %y\tTwo-digit year.\t                18\n",
    "        %H  is a 24-hour clock\n",
    "        \n",
    "- you can use parse dates when the information is in mulitple columns:\n",
    "    parse_dates=[['Date', 'Time']]\n",
    "    pd.to_datetime(df['Date'] + ' ' + df['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STDIN READ when .tsv like data Data is expected\n",
    "- zero line = number of samples in data set\n",
    "- first line = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=int(sys.stdin.readline())\n",
    "my_header = sys.stdin.readline().split()\n",
    "\n",
    "data = sys.stdin.read().splitlines()\n",
    "data = [re.split(r'\\t', l) for l in data]\n",
    "df = pd.DataFrame(data, columns= my_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case filedata needs to be located \n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "print(dir_path)\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "files = os.listdir(os.curdir)\n",
    "print(files)\n",
    "\n",
    "# In case data resides in its own directory below\n",
    "os.chdir(path)\n",
    "\n",
    "# Verify the data-file has been located\n",
    "fname = \"\"\n",
    "with open(fname, 'r') as fin:\n",
    "    print(fin.read())\n",
    "    \n",
    "# Load the data into a dataframe\n",
    "data = pd.read_csv(r'the_path_in_the_remote_machine/fname', sep='\\t',\n",
    "                   index_col = 0, header = 1, names = ['charge_time','battery_time'],\n",
    "                   parse_dates = [0], infer_datetime_format = True, date_parser = pd.to_datetime,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use fillna to impute the missing values\n",
    "df_bank['job'].fillna(df_bank['job'].value_counts().idxmax(), inplace=True)\n",
    "df_bank['marital'].fillna(df_bank['marital'].value_counts().idxmax(), inplace=True)\n",
    "df_bank['duration'].fillna(mode(df_bank['duration']).mode[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Series for Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "smt.seasonal_decompose(df_pass).plot()\n",
    "plt.gcf().set_size_inches(10, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: used to improve algorithm performance and/or improve robustness\n",
    "  - Bootstrapping: central concept: random sampling with replacement. Gives model a chance to learn the vairous biases, variances and features within the data set -- even applicable in small data sets. With the increase in processing power, it is much more possible to run these multiple models in parallel than ever before. Two methodologies are: Boosting and bagging\n",
    "  - Bagging: run multiple prediction models in parallel and aggregate their output. Helps reduce variance in cases where overfitting is a concern. Aggregation can take the form of voting (classification) or averaging (regression).\n",
    "  - Boosting: same but you weight the model's output. Usually, start by running models with equal weights on the first pass. The model then keeps track of which samples are the most frequently miss-classified and gives them heavier weights -- requiring more iteration to properly train. The model error rates are also kept track of and the better models are given more weight. Boosting is most likely to pick the better of the models included in the ensemble. It can also reduce the bias in an underfit model.\n",
    "<br>\n",
    "- **Summary:** Booth boosting and bagging are techniques to decrease variance -- this is why most Kaggle winners use this type of approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Algorithms\n",
    "https://brilliant.org/wiki/sorting-algorithms/\n",
    "![title](img/Sorting_algos.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Regularization?\n",
    "[~] A way to reduce overfitting is penalize higher degree polynomials. This ensures that a higher degree polynomial is selected only if it reduces the error significantly compared to a simpler model, to overcome the penalty.  \n",
    "\n",
    "[~] Occam's razor (or Ockham's razor) is a principle from philosophy. Suppose there exist two explanations for an occurrence. In this case the simplier one is usually better. Another way of saying it is that the more assumptions you have to make, the more unlikely an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients of Variation (like a k-factor)\n",
    "In probability theory and statistics, the coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. It is often expressed as a percentage, and is defined as the ratio of the standard deviation to the mean. Also used to measure volatility of a security in finance\n",
    "\n",
    "Advantages\n",
    "The coefficient of variation is useful because the standard deviation of data must always be understood in the context of the mean of the data. In contrast, the actual value of the CV is independent of the unit in which the measurement has been taken, so it is a dimensionless number. For comparison between data sets with different units or widely different means, one should use the coefficient of variation instead of the standard deviation.\n",
    "\n",
    "Disadvantages\n",
    "When the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Convex Optimizaiton Methods\n",
    "\n",
    "non-convex optimization is at least NP-hard -- worried about multiple saddle points and local minima.  Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Engine\n",
    "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/\n",
    "A recommendation engine filters the data using different algorithms and recommends the most relevant items to users. It first captures the past behavior of a customer and based on that, recommends products which the users might be likely to buy.\n",
    "\n",
    "- We can recommend items to a user which are most popular among all the users\n",
    "- We can divide the users into multiple segments based on their preferences (user features) and recommend items to them based on the segment they belong to\n",
    "\n",
    "2.3.1 Content filtering (e.g. similarty of terms -cosine distance)\n",
    "2.3.2 Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
